---
title: "Project: Weight Lifting Exercises Dataset"
author: "N.B."
date: "Monday, September 21, 2015"
output: html_document
---

## Summary

The WLE dataset was collected in order to determine how well an activity was
perfomed. Detailed information on the data collection is described in
the following paper: 
<br><br><a class="colaborador" href="http://groupware.les.inf.puc-rio.br/collaborator.jsf?p1=evelloso">Velloso, E.</a>; Bulling, A.; Gellersen, H.; <a class="colaborador" href="http://groupware.les.inf.puc-rio.br/collaborator.jsf?p1=ugulino">Ugulino, W.</a>; <a class="colaborador" href="http://groupware.les.inf.puc-rio.br/collaborator.jsf?p1=hugo">Fuks, H.</a> <strong><a class="publicacao" href="http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201" title="Qualitative Activity Recognition of Weight Lifting Exercises">Qualitative Activity Recognition of Weight Lifting Exercises</a></strong>. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

## Aim of the project

The question we aim to answer is: How well were the exercises performed?
Correctly or incorrectly? If the exercise was not performed correctly,
we also aim to determine in what fashion the exercise was performed
incorrectly. 

## Data

Data was collected from six participants, who were asked to perform repetitions of dumbbell bicep curls. The participants were equipped with four sensors, located on the belt, arm, forearm and dumbbell, and were asked to perform the curls in five fashions, one correctly (class A), and four incorrect fashions (classes B, C, D, and E). For further details on the specifications, we
refer the reader to the published paper. In total, 160 variables are
reported, including the classe variable. 

```{r echo=FALSE, results="hide"}
#Load necessary libraries and training data:
library(caret)
#library(rattle)
wldata <- read.csv(file="pml-training.csv")

#Will first examine GLM models. Since these can only use 2-class outcomes:
wldata$classe <- as.character(wldata$classe)
wldata$classe[wldata$classe == "C"] <- "B"
wldata$classe[wldata$classe == "D"] <- "B"
wldata$classe[wldata$classe == "E"] <- "B"
wldata$classe <- as.factor(wldata$classe) 
```

## Feature Selection

Features were first examined by sensor. A total of 52 features were selected for the analysis as outlined below. Of these 52 features we have identified 41 principal components.

```{r echo=FALSE, results="hide"}
set.seed(33456)
#Create testing and training partitions, and select relevant features:

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
training <- wldata[inTrain,]
testing <- wldata[-inTrain,]

armset <- grep("_arm", names(training))
train_arm <- training[,c(1,armset[1:length(armset)],160)]

beltset <- grep("belt", names(training))
train_belt <- training[,c(1,beltset[1:length(beltset)],160)]

dumbset <- grep("dumbbell", names(training))
train_dumb <- training[,c(1,dumbset[1:length(dumbset)],160)]

foreset <- grep("forearm", names(training))
train_fore <- training[,c(1,foreset[1:length(foreset)],160)]

#summary(train_arm)
train_arm_f <- train_arm[,c(1,2,3,4,5,16,17,18,19,20,21,22,23,24,40)]

M <- abs(cor(train_arm_f[,-15]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)

#summary(train_belt)
train_belt_f <- train_belt[,c(1,2,3,4,5,31,32,33, 34, 35, 36, 37, 38,39,40)]

M <- abs(cor(train_belt_f[,-15]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)

#summary(train_dumb)
train_dumb_f <- train_dumb[,c(1,2,3,4,20,31,32,33,34,35,36,37,38,39,40)]

M <- abs(cor(train_dumb_f[,-15]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)

#summary(train_fore)
train_fore_f <- train_fore[,c(1,2,3,4,20,31,32,33,34,35,36,37,38,39,40)]

M <- abs(cor(train_fore_f[,-15]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)

total <- merge(train_fore_f,train_arm_f,by="X")
total2 <- merge(total, train_dumb_f, by = "X")
total2x <- total2[,-15]
total2x <- total2x[,-28]
total2x <- total2x[,-41]
train_setx <- merge(total2x,train_belt_f, by = "X")

train_set <- train_setx[,-1]

SelectedFeatures <- c(122,123,124,140,151,152,153,154,155,156,157,158,159,46,47,48,49,60,61,62,63,64,65,66,67,68,84,85,86,102,113,114,115,116,117,118,119,120,121,8,9,10,11,37,38,39,40,41,42,43,44,45,160)

test_set <- testing[,SelectedFeatures]
```

### Arm

39 variables are associated to the arm sensor. Of these, mostly are
statistics derived from other variables, and were thus discarded. 13
features remained. Of these we found three pairs of highly correlated
features, as so were able to compress the number of arm features to 10.

### Forearm

Similarly to the arm sensor, the 39 associated features were reduced to 13. Of
these, in our training sets we did not detect any highly correlated
features.

### Belt

Once again, the 39 associated features were reduced to 13. In the belt
detected two sets of highly correlated features: one set of five
features, and one set of three features. We were thus able to compress
the number of features for the belt to seven features. 

### Dumbbell

The 39 associated features were reduced to 13, and further reduced to 11
features as two pairs of highly correlated features were detected.


## Model Construction

### PCA/GLM models

As a first attempt at model construction a PCA/GLM approach was used. The
41 principal components discussed in the section on feature selection
were used in the analysis. This approach was somewhat limited, as it
only allowed for a binary outcome, and so infomation was lost regarding
how an exercise was incorrectly perfomed. 

PCA/GLM model construction from selected features

Cross validation sampling - 1

```{r}
preProc <- preProcess( train_set[,-53],method="pca",pcaComp=41)
trainPC <- predict(preProc,train_set[,-53])
modelFitPCA <- train(train_set$classe ~ .,method="glm",data=trainPC)

testPC <- predict(preProc,test_set[,-53])
confusionMatrix(test_set$classe,predict(modelFitPCA,testPC))
```

PCA/GLM model construction from selected features

Cross validation sampling - 2

```{r}
set.seed(5567)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
training <- wldata[inTrain,SelectedFeatures]
testing <- wldata[-inTrain,SelectedFeatures]

preProc <- preProcess( training[,-53],method="pca",pcaComp=41)
trainPC <- predict(preProc,training[,-53])
modelFitPCA <- train(training$classe ~ .,method="glm",data=trainPC)

testPC <- predict(preProc,testing[,-53])
confusionMatrix(testing$classe,predict(modelFitPCA,testPC))
```

PCA/GLM model construction from selected features

Cross validation sampling - 3

```{r results="hide"}
set.seed(13245)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
train_set <- wldata[inTrain,SelectedFeatures]
test_set <- wldata[-inTrain,SelectedFeatures]

preProc <- preProcess( train_set[,-53],method="pca",pcaComp=41)
trainPC <- predict(preProc,train_set[,-53])
modelFitPCA <- train(train_set$classe ~ .,method="glm",data=trainPC)
```
```{r}
testPC <- predict(preProc,test_set[,-53])
confusionMatrix(test_set$classe,predict(modelFitPCA,testPC))
```

PCA/GLM model construction from selected features

Cross validation sampling - 4

```{r}
set.seed(1125)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
train_set <- wldata[inTrain,SelectedFeatures]
test_set <- wldata[-inTrain,SelectedFeatures]

preProc <- preProcess( train_set[,-53],method="pca",pcaComp=41)
trainPC <- predict(preProc,train_set[,-53])
modelFitPCA <- train(train_set$classe ~ .,method="glm",data=trainPC)

testPC <- predict(preProc,test_set[,-53])
confusionMatrix(test_set$classe,predict(modelFitPCA,testPC))
```


### Classification Tree Models

In our next appoach, we examined a classification tree approach.
As a first approximaton to estimate the accuracy of the approach we
classified a binary outcome for the classe variable. "A" (correct) or
"not A". The accuracy was not sufficent, and so a third model approach
was examined. 

Classification Tree model construction from selected features

Cross validation sampling - 1

```{r}
set.seed(53748)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
train_set <- wldata[inTrain,SelectedFeatures]
test_set <- wldata[-inTrain,SelectedFeatures]

modFit <- train(classe ~ .,method="rpart",data=train_set)
#print(modFit$finalModel)
preTree <- predict(modFit,newdata=test_set)
confusionMatrix(test_set$classe,preTree)
```

Classification Tree model construction from selected features

Cross validation sampling - 2

```{r}
set.seed(64785)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
train_set <- wldata[inTrain,SelectedFeatures]
test_set <- wldata[-inTrain,SelectedFeatures]

modFit <- train(classe ~ .,method="rpart",data=train_set)
#print(modFit$finalModel)
preTree <- predict(modFit,newdata=test_set)
confusionMatrix(test_set$classe,preTree)
```

Classification Tree model construction from selected features

Cross validation sampling - 3

```{r}
set.seed(4758)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
train_set <- wldata[inTrain,SelectedFeatures]
test_set <- wldata[-inTrain,SelectedFeatures]

modFit <- train(classe ~ .,method="rpart",data=train_set)
#print(modFit$finalModel)
preTree <- predict(modFit,newdata=test_set)
confusionMatrix(test_set$classe,preTree)
```

Classification Tree model construction from selected features

Cross validation sampling - 4

```{r}
set.seed(5)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
train_set <- wldata[inTrain,SelectedFeatures]
test_set <- wldata[-inTrain,SelectedFeatures]

modFit <- train(classe ~ .,method="rpart",data=train_set)
#print(modFit$finalModel)
preTree <- predict(modFit,newdata=test_set)
confusionMatrix(test_set$classe,preTree)
```

### Random Forest

In our final model, a random forest approach was used. Although much more computationally intensive, this approach proved to give a much smaller expected out of sample error. Here are the results from four runs using the full classe information. 

Random Forest

Cross validation sampling - 1

```{r}
#Reload data to recover original class information
wldata <- read.csv(file="pml-training.csv")
set.seed(645)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
train_set <- wldata[inTrain,SelectedFeatures]
test_set <- wldata[-inTrain,SelectedFeatures]

modFit_rf <- train(classe~ .,data=train_set,method="rf",prox=TRUE)

pred_rf <- predict(modFit_rf,test_set) 
confusionMatrix(test_set$classe,pred_rf)
```

Random Forest

Cross validation sampling - 2

```{r}
set.seed(6748)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
train_set <- wldata[inTrain,SelectedFeatures]
test_set <- wldata[-inTrain,SelectedFeatures]

modFit_rf2 <- train(classe~ .,data=train_set,method="rf",prox=TRUE)

pred_rf2 <- predict(modFit_rf2,test_set) 
confusionMatrix(test_set$classe,pred_rf2)
```

Random Forest

Cross validation sampling - 3

```{r}
set.seed(2424)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
train_set <- wldata[inTrain,SelectedFeatures]
test_set <- wldata[-inTrain,SelectedFeatures]

modFit_rf3 <- train(classe~ .,data=train_set,method="rf",prox=TRUE)

pred_rf3 <- predict(modFit_rf3,test_set) 
confusionMatrix(test_set$classe,pred_rf3)
```

```{r echo=FALSE, results="hide"}
wldata_test <- read.csv(file="pml-testing.csv")
wldata_testx <- wldata_test[,SelectedFeatures]
pred_rf_test <- predict(modFit_rf3,wldata_testx) 
write.table(pred_rf_test,file="answers.Rda")
```

Random Forest

Cross validation sampling - 4

```{r}
set.seed(5142)

inTrain <- createDataPartition(y=wldata$classe, p=0.60, list=FALSE)
train_set <- wldata[inTrain,SelectedFeatures]
test_set <- wldata[-inTrain,SelectedFeatures]

modFit_rf4 <- train(classe~ .,data=train_set,method="rf",prox=TRUE)

pred_rf4 <- predict(modFit_rf4,test_set) 
confusionMatrix(test_set$classe,pred_rf4)
```

## Cross Validation

In each of the above described model constructions, the training set was
resampled four times into a training and a testing subset. A
different seed was specified for each resampling. The out-of-sample
errors reported in the summary section are the mean values obtained
from the cross-validation.

## Summary & Discussion

The expected out-of-sample errors for the three models discussed are reported in the table below. The error calculated for each cross validation run is
shown, along with the mean value per model. 

### Expected Out-of-Sample Errors

<table style="text-align: left; height: 172px; width: 432px; margin-left: 38px; background-color: rgb(255, 255, 204);" border="1" cellpadding="2" cellspacing="2"><tbody><tr><td style="width: 165px;">Out-of-sample-error (%)</td><td>PCA</td><td>Rpart</td><td>RF</td></tr><tr><td style="width: 165px;">CV 1</td><td>13.79</td><td>17.34</td><td>0.46</td></tr><tr><td style="width: 165px;">CV 2</td><td>14.22</td><td>17.83</td><td>0.79</td></tr><tr><td style="width: 165px;">CV 3</td><td>13.19</td><td>17.35</td><td>0.92</td></tr><tr><td style="width: 165px;">CV 4</td><td>14.30</td><td>17.09</td><td>0.68</td></tr><tr><td style="width: 165px;">Mean</td><td>13.88</td><td>17.40</td><td>0.71</td></tr></tbody></table>


The first two models presented were chosen while trying to achieve a
balance between accuracy and computational load. However, it quickly
became clear that a sufficient amount of accuracy could not be obtained
with these models. The first initial tests using the random forest
approach were very promising, and so this
model was chosen as the final testing model which included the full class information. Our expected out of sample error is: 0.71%.
